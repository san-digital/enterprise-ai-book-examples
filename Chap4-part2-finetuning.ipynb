{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac772ebc-b35b-48ad-9521-879ba3f7c237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Chapter 4 Part 2: Fine-Tuning\n",
       "\n",
       "This section walks you through generating an example fine tuned model for use in a \"Biscuit selector\" app.\n",
       "\n",
       "**Goal**: Teach the LLM model your biscuit preferences with a number of examples.\n",
       "\n",
       "The training set has been generated with the following assumption\n",
       "- Tired = chocolate.\n",
       "- Bored = novelty, maybe jammy dodger or tunnock tea cake.\n",
       "- Sad = fancy biscuit.\n",
       "- Hungry = substantial high calorie, hobnob.\n",
       "\n",
       "Once model is generated, run the biscuit selection app to test the fine tuned model.\n",
       "```sh\n",
       "cd ../chapter-4/biscuit-selector-app/\n",
       "yarn \n",
       "yarn dev\n",
       "```\n",
       "\n",
       "Within the app, recommendation preferences can be logged and used to update the model. So the model learns your preferences without them being explicitly defined.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "display(Markdown('''# Chapter 4 Part 2: Fine-Tuning\n",
    "\n",
    "This section walks you through generating an example fine tuned model for use in a \"Biscuit selector\" app.\n",
    "\n",
    "**Goal**: Teach the LLM model your biscuit preferences with a number of examples.\n",
    "\n",
    "The training set has been generated with the following assumption\n",
    "- Tired = chocolate.\n",
    "- Bored = novelty, maybe jammy dodger or tunnock tea cake.\n",
    "- Sad = fancy biscuit.\n",
    "- Hungry = substantial high calorie, hobnob.\n",
    "\n",
    "Once model is generated, run the biscuit selection app to test the fine tuned model.\n",
    "```sh\n",
    "cd ../chapter-4/biscuit-selector-app/\n",
    "yarn \n",
    "yarn dev\n",
    "```\n",
    "\n",
    "Within the app, recommendation preferences can be logged and used to update the model. So the model learns your preferences without them being explicitly defined.\n",
    "'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6e75f47-ce3b-465d-8ebd-6d670835a791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading training file...\n",
      "File uploaded: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "Starting fine-tuning job...\n",
      "Fine-tune job started: ftjob-O9XnArvtgB0YQfSvUlWTK6l5\n",
      "⏳ Waiting for fine-tuning job to complete...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     73\u001b[39m file_id = upload_training_file(\u001b[33m\"\u001b[39m\u001b[33mchapter-4/resources/biscuit_selector.jsonl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     74\u001b[39m job_id = start_fine_tune(file_id)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m result = \u001b[43mwait_for_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinal status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.status == \u001b[33m\"\u001b[39m\u001b[33msucceeded\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m result.fine_tuned_model:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mwait_for_completion\u001b[39m\u001b[34m(job_id, poll_interval)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     43\u001b[39m     job = openai.fine_tuning.jobs.retrieve(job_id)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     elapsed = \u001b[38;5;28mstr\u001b[39m(\u001b[43mdatetime\u001b[49m.timedelta(seconds=\u001b[38;5;28mint\u001b[39m(time.time() - start_time)))\n\u001b[32m     45\u001b[39m     step += \u001b[32m1\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Elapsed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # or: openai.api_key = \"sk-...\"\n",
    "\n",
    "def upload_training_file(file_path: str):\n",
    "    print(\"Uploading training file...\")\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        response = openai.files.create(file=f, purpose=\"fine-tune\")\n",
    "    print(f\"File uploaded: {response.id}\")\n",
    "    return response.id\n",
    "\n",
    "def start_fine_tune(file_id: str, model=\"gpt-4.1-mini-2025-04-14\"):\n",
    "    print(\"Starting fine-tuning job...\")\n",
    "    response = openai.fine_tuning.jobs.create(training_file=file_id, model=model)\n",
    "    print(f\"Fine-tune job started: {response.id}\")\n",
    "    return response.id\n",
    "\n",
    "# === MAIN ===\n",
    "if __name__ == \"__main__\":     # ⬅ Intermediate file\n",
    "\n",
    "    file_id = upload_training_file(\"chapter-4/resources/biscuit_selector.jsonl\")\n",
    "    job_id = start_fine_tune(file_id)\n",
    "    print(\"Started job\")\n",
    "    print(job_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8691d2c2-5db7-4aba-972a-8573c8a8117d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Waiting for fine-tuning job to complete...\n",
      "\n",
      "[1] Elapsed: 0:00:00 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "    [event @ 2025-07-28 16:40:21] Step 216/300: training loss=0.49\n",
      "    [event @ 2025-07-28 16:40:21] Step 217/300: training loss=0.32\n",
      "    [event @ 2025-07-28 16:40:21] Step 218/300: training loss=0.30\n",
      "    [event @ 2025-07-28 16:40:21] Step 219/300: training loss=0.55\n",
      "    [event @ 2025-07-28 16:40:26] Step 220/300: training loss=0.26\n",
      "    [event @ 2025-07-28 16:40:28] Step 221/300: training loss=0.26\n",
      "    [event @ 2025-07-28 16:40:28] Step 222/300: training loss=0.25\n",
      "    [event @ 2025-07-28 16:40:28] Step 223/300: training loss=0.70\n",
      "    [event @ 2025-07-28 16:40:28] Step 224/300: training loss=0.48\n",
      "    [event @ 2025-07-28 16:40:28] Step 225/300: training loss=0.24\n",
      "    [event @ 2025-07-28 16:40:28] Step 226/300: training loss=0.24\n",
      "    [event @ 2025-07-28 16:40:33] Step 227/300: training loss=0.19\n",
      "    [event @ 2025-07-28 16:40:34] Step 228/300: training loss=0.16\n",
      "    [event @ 2025-07-28 16:40:34] Step 229/300: training loss=0.93\n",
      "    [event @ 2025-07-28 16:40:34] Step 230/300: training loss=0.42\n",
      "    [event @ 2025-07-28 16:40:35] Step 231/300: training loss=0.18\n",
      "    [event @ 2025-07-28 16:40:35] Step 232/300: training loss=1.46\n",
      "    [event @ 2025-07-28 16:40:35] Step 233/300: training loss=0.44\n",
      "    [event @ 2025-07-28 16:40:42] Step 234/300: training loss=0.64\n",
      "    [event @ 2025-07-28 16:40:43] Step 235/300: training loss=0.14\n",
      "    [event @ 2025-07-28 16:40:43] Step 236/300: training loss=0.16\n",
      "    [event @ 2025-07-28 16:40:43] Step 237/300: training loss=0.17\n",
      "    [event @ 2025-07-28 16:40:43] Step 238/300: training loss=0.52\n",
      "    [event @ 2025-07-28 16:40:43] Step 239/300: training loss=0.61\n",
      "    [event @ 2025-07-28 16:40:43] Step 240/300: training loss=0.35\n",
      "    [event @ 2025-07-28 16:40:43] Step 241/300: training loss=0.46\n",
      "    [event @ 2025-07-28 16:40:43] Step 242/300: training loss=0.04\n",
      "    [event @ 2025-07-28 16:40:49] Step 243/300: training loss=1.22\n",
      "    [event @ 2025-07-28 16:40:50] Step 244/300: training loss=0.44\n",
      "    [event @ 2025-07-28 16:40:50] Step 245/300: training loss=0.04\n",
      "    [event @ 2025-07-28 16:40:50] Step 246/300: training loss=1.55\n",
      "    [event @ 2025-07-28 16:40:50] Step 247/300: training loss=0.24\n",
      "    [event @ 2025-07-28 16:40:50] Step 248/300: training loss=0.02\n",
      "    [event @ 2025-07-28 16:40:50] Step 249/300: training loss=0.18\n",
      "    [event @ 2025-07-28 16:40:50] Step 250/300: training loss=1.20\n",
      "    [event @ 2025-07-28 16:40:56] Step 251/300: training loss=0.98\n",
      "    [event @ 2025-07-28 16:40:57] Step 252/300: training loss=0.87\n",
      "    [event @ 2025-07-28 16:40:57] Step 253/300: training loss=0.27\n",
      "    [event @ 2025-07-28 16:40:57] Step 254/300: training loss=1.44\n",
      "    [event @ 2025-07-28 16:40:57] Step 255/300: training loss=1.92\n",
      "    [event @ 2025-07-28 16:40:57] Step 256/300: training loss=0.37\n",
      "    [event @ 2025-07-28 16:40:57] Step 257/300: training loss=1.02\n",
      "    [event @ 2025-07-28 16:40:57] Step 258/300: training loss=0.14\n",
      "    [event @ 2025-07-28 16:41:02] Step 259/300: training loss=0.50\n",
      "    [event @ 2025-07-28 16:41:03] Step 260/300: training loss=0.20\n",
      "    [event @ 2025-07-28 16:41:04] Step 261/300: training loss=0.41\n",
      "    [event @ 2025-07-28 16:41:04] Step 262/300: training loss=0.11\n",
      "    [event @ 2025-07-28 16:41:04] Step 263/300: training loss=1.04\n",
      "    [event @ 2025-07-28 16:41:04] Step 264/300: training loss=0.13\n",
      "    [event @ 2025-07-28 16:41:09] Step 265/300: training loss=1.16\n",
      "[2] Elapsed: 0:00:30 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "    [event @ 2025-07-28 16:41:10] Step 266/300: training loss=0.42\n",
      "    [event @ 2025-07-28 16:41:10] Step 267/300: training loss=0.29\n",
      "    [event @ 2025-07-28 16:41:10] Step 268/300: training loss=0.50\n",
      "    [event @ 2025-07-28 16:41:10] Step 269/300: training loss=0.11\n",
      "    [event @ 2025-07-28 16:41:10] Step 270/300: training loss=0.62\n",
      "    [event @ 2025-07-28 16:41:11] Step 271/300: training loss=0.34\n",
      "    [event @ 2025-07-28 16:41:11] Step 272/300: training loss=0.34\n",
      "    [event @ 2025-07-28 16:41:17] Step 273/300: training loss=0.80\n",
      "    [event @ 2025-07-28 16:41:18] Step 274/300: training loss=1.31\n",
      "    [event @ 2025-07-28 16:41:18] Step 275/300: training loss=0.44\n",
      "    [event @ 2025-07-28 16:41:18] Step 276/300: training loss=0.42\n",
      "    [event @ 2025-07-28 16:41:18] Step 277/300: training loss=0.43\n",
      "    [event @ 2025-07-28 16:41:18] Step 278/300: training loss=0.12\n",
      "    [event @ 2025-07-28 16:41:18] Step 279/300: training loss=0.27\n",
      "    [event @ 2025-07-28 16:41:18] Step 280/300: training loss=0.38\n",
      "    [event @ 2025-07-28 16:41:19] Step 281/300: training loss=0.47\n",
      "    [event @ 2025-07-28 16:41:19] Step 282/300: training loss=0.32\n",
      "    [event @ 2025-07-28 16:41:24] Step 283/300: training loss=0.29\n",
      "    [event @ 2025-07-28 16:41:25] Step 284/300: training loss=0.40\n",
      "    [event @ 2025-07-28 16:41:25] Step 285/300: training loss=0.17\n",
      "    [event @ 2025-07-28 16:41:25] Step 286/300: training loss=0.44\n",
      "    [event @ 2025-07-28 16:41:25] Step 287/300: training loss=0.40\n",
      "    [event @ 2025-07-28 16:41:25] Step 288/300: training loss=0.38\n",
      "    [event @ 2025-07-28 16:41:25] Step 289/300: training loss=0.53\n",
      "    [event @ 2025-07-28 16:41:30] Step 290/300: training loss=0.32\n",
      "    [event @ 2025-07-28 16:41:32] Step 291/300: training loss=0.42\n",
      "    [event @ 2025-07-28 16:41:32] Step 292/300: training loss=0.52\n",
      "    [event @ 2025-07-28 16:41:32] Step 293/300: training loss=0.39\n",
      "    [event @ 2025-07-28 16:41:32] Step 294/300: training loss=0.48\n",
      "    [event @ 2025-07-28 16:41:32] Step 295/300: training loss=0.46\n",
      "    [event @ 2025-07-28 16:41:32] Step 296/300: training loss=0.32\n",
      "    [event @ 2025-07-28 16:41:37] Step 297/300: training loss=0.40\n",
      "[3] Elapsed: 0:01:01 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[4] Elapsed: 0:01:32 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "    [event @ 2025-07-28 16:41:38] Step 298/300: training loss=0.70\n",
      "    [event @ 2025-07-28 16:41:38] Step 299/300: training loss=0.54\n",
      "    [event @ 2025-07-28 16:41:38] Step 300/300: training loss=0.10\n",
      "    [event @ 2025-07-28 16:42:16] Checkpoint created at step 100\n",
      "    [event @ 2025-07-28 16:42:16] Checkpoint created at step 200\n",
      "    [event @ 2025-07-28 16:42:16] New fine-tuned model created\n",
      "    [event @ 2025-07-28 16:42:16] Evaluating model against our usage policies\n",
      "[5] Elapsed: 0:02:03 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[6] Elapsed: 0:02:34 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[7] Elapsed: 0:03:05 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[8] Elapsed: 0:03:35 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[9] Elapsed: 0:04:06 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[10] Elapsed: 0:04:37 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[11] Elapsed: 0:05:07 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[12] Elapsed: 0:05:39 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[13] Elapsed: 0:06:10 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[14] Elapsed: 0:06:41 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[15] Elapsed: 0:07:12 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[16] Elapsed: 0:07:43 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[17] Elapsed: 0:08:13 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[18] Elapsed: 0:08:44 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[19] Elapsed: 0:09:15 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[20] Elapsed: 0:09:46 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[21] Elapsed: 0:10:16 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[22] Elapsed: 0:10:47 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[23] Elapsed: 0:11:18 | Status: running\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    - Output fine-tuned model name: N/A\n",
      "[24] Elapsed: 0:11:49 | Status: succeeded\n",
      "    - Model: gpt-4.1-mini-2025-04-14\n",
      "    - Training File: file-SnnBWeGCCnMPm7TqF6mVFV\n",
      "    [event @ 2025-07-28 16:52:47] Moderation checks for snapshot ft:gpt-4.1-mini-2025-04-14:san-digital::ByKHPYIU passed.\n",
      "    [event @ 2025-07-28 16:52:47] Usage policy evaluations completed, model is now enabled for sampling\n",
      "    [event @ 2025-07-28 16:52:52] The job has successfully completed\n",
      "\n",
      "✅ Job SUCCEEDED\n",
      "Final status: succeeded\n",
      "Saving fine-tuned model name to chapter-4/resources/model.json\n",
      "Model name saved.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "import openai\n",
    "import os\n",
    "\n",
    "\n",
    "def save_model_name(model_name: str, output_path: str):\n",
    "    print(f\"Saving fine-tuned model name to {output_path}\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump({\"model\": model_name}, f)\n",
    "    print(\"Model name saved.\")\n",
    "\n",
    "\n",
    "def wait_for_completion(job_id: str, poll_interval=30):\n",
    "    print(\"⏳ Waiting for fine-tuning job to complete...\\n\")\n",
    "    start_time = time.time()\n",
    "    seen_event_ids = set()\n",
    "    step = 0\n",
    "\n",
    "    while True:\n",
    "        job = openai.fine_tuning.jobs.retrieve(job_id)\n",
    "        elapsed = str(datetime.timedelta(seconds=int(time.time() - start_time)))\n",
    "        step += 1\n",
    "\n",
    "        print(f\"[{step}] Elapsed: {elapsed} | Status: {job.status}\")\n",
    "        print(f\"    - Model: {job.model}\")\n",
    "        print(f\"    - Training File: {job.training_file}\")\n",
    "        if job.validation_file:\n",
    "            print(f\"    - Validation File: {job.validation_file}\")\n",
    "        if job.status == \"running\":\n",
    "            print(f\"    - Output fine-tuned model name: {job.fine_tuned_model or 'N/A'}\")\n",
    "\n",
    "        # ✅ Correct attribute access for event objects\n",
    "        try:\n",
    "            events = openai.fine_tuning.jobs.list_events(job_id, limit=50).data\n",
    "            new_events = [e for e in reversed(events) if e.id not in seen_event_ids]\n",
    "            for event in new_events:\n",
    "                seen_event_ids.add(event.id)\n",
    "                ts = datetime.datetime.fromtimestamp(event.created_at).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(f\"    [event @ {ts}] {event.message}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠️ Failed to retrieve events: {e}\")\n",
    "\n",
    "        if job.status in [\"succeeded\", \"failed\", \"cancelled\"]:\n",
    "            print(f\"\\n✅ Job {job.status.upper()}\")\n",
    "            return job\n",
    "\n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "model_output_path = \"chapter-4/resources/model.json\"\n",
    "\n",
    "result = wait_for_completion(\"ftjob-O9XnArvtgB0YQfSvUlWTK6l5\")\n",
    "\n",
    "print(f\"Final status: {result.status}\")\n",
    "if result.status == \"succeeded\" and result.fine_tuned_model:\n",
    "    save_model_name(result.fine_tuned_model, model_output_path)\n",
    "else:\n",
    "    print(\"Fine-tuning failed or no model was produced.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3cabb2-992b-444c-b461-5cf02788bdc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf04ef-09e0-4649-b103-a0386310f71e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
